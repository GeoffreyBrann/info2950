{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
<<<<<<< HEAD
    "### Model Selection Process\n",
    "\n",
    "First, data from each year was pooled into a comprehensive dataframe called data_all, that included all years, and each player from each year as well as their respective offensive statistics. Then we had to get rid of two NaN rows present in the data for some reason for all the models to ensure that the code would compile when we needed to run the regressions. \n",
    "\n",
    "Once the data was combined and cleaned into a presentable dataframe, we needed to generate scatterplots that pitted the response variable of interest against all the other offensive statistics. First, we needed to create a list of all the possible predictors that we wanted to compare the response variable to and as a result we did just that and created a list of all the offensive predictors, minus the response, and saved it to a variable \"predictors\". We then used this list to create a for loop that would loop through each predictor and create a scatterplot comparing the specifc predictor and the response varible.\n",
    "\n",
    "Once all the scatterplots were generated, we went through all of them and chose which ones had any sort of noticeable relationship/association and saved these predictors to an array labelled X.\n",
    "\n",
    "Once we had our array of what we thought were significant predictors we then fitted the multiple linear model using the LinearRegression() function from sklearns and imported the statsmodels.api package to help give us p-values and coefficeints of the fitted model. We then examined the list of p-values for each predictor and looked for ones that had p-values below 0.05 so that we would know which ones we should try and remove. \n",
    "\n",
    "After we had an idea of which predictors were insignificant, we would create a reduced multiple linear model without the insignificant predictors and run an anova test which compared the reduced model to the original model with all predictors, at which point we would look at the p-value of the anova output and see if it was greater than 0.05. \n",
    "\n",
    "If the anova output showed a p-value greater than 0.05 it was safe to remove all the insignificant predictors simultaneously and write down the final model. However, if the anova summary gave a p-value greater than 0.05 we would need to conduct further analyses and run more anova tests by removing one insignificant predictor at a time until a significant model was reached. An example of a more detailed walkthrough of this process can be found in the later sections of the OPS+ model page. \n",
    "\n",
    "### Final OPS Model\n",
    "\n",
    "##########################\n",
    "\n",
    "OPS_Plus ~ exit_velocity_avg + launch_angle_avg + barrels + poorlytopped_percent + hard_hit_percent + oz_swing_percent + pitch_count + in_zone + groundballs_percent + flyballs_percent\n",
    "\n",
    "##########################\n",
    "\n",
    "### Final Sprint Speed Model\n",
    "\n",
    "##########################\n",
    "\n",
    "sprint_speed ~ solidcontact_percent + flyballs_percent + f_strike_percent + n_bolts + UBR\n",
    "\n",
    "##########################\n",
    "\n",
    "### Final UBR Model\n",
    "\n",
    "###########################\n",
    "\n",
    "Final Model:\n",
    "\n",
    "UBR ~ exit_velocity_avg + n_bolts + sprint_speed\n",
    "###########################"
=======
    "wRC+ Final Model: wRC_Plus ~ exit_velocity_avg + launch_angle_avg + barrels + poorlytopped_percent + hard_hit_percent + oz_swing_percent + out_zone + edge + groundballs_percent + flyballs_percent"
>>>>>>> 2a2d47db753918d0f87dcd92bd62143e7fbf7531
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "wOBA Final Model: xwOBA ~ exit_velocity_avg + launch_angle_avg + barrels + solidcontact_percent + poorlytopped_percent + hard_hit_percent + out_zone + oz_swing_percent + edge + groundballs_percent + flyballs_percent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Our Model Selection Process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we combined the data from all five years (2015-2019) to form one pooled dataset in order to have the largest possible sample size for our analysis. We made a list of all predictor variables of interest which we could possibly include in our models. Then we examined scatterplots of the response variable (OPS+ or wOBA, etc.) vs. each predictor. After studying the scatterplots, we created an initial model with the predictors which seemed to have a significant relationship with the response. In this part, we were fairly liberal in adding predictors to the model, except that we avoided variables which were clearly redundant (barrels and barrel_batted_rate, for example). After we obtained our initial model, we tested its significance by obtaining the t-values and p-values of each predictor's slope coefficient. We then created a reduced model, removing terms which seemed to be unnecessary based on their p-values. We tested if there was any significant difference between the reduced model and the original model using an ANOVA test. If there did appear to be a significant difference between the two models based on the ANOVA test, then we created a larger reduced model in which we added one of the originally removed predictors to the original reduced model. If we concluded based on the ANOVA test that there was no significant difference between the two models, then we could proceed with the reduced model. We then examined the t-values and p-values of the reduced model. If all terms seemed to be necessary, we kept this model as the final model. If there were some terms which might not have been necessary, we continued with the process described above, creating another reduced model and performing another ANOVA test. We continued to do these steps until we found a final model in which all the predictors were necessary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}